Task 1: Introduction

first we run this to import dataset from "!git clone https://github.com/am1tyadav/superhero"

here we open the file then read the file after which we store it in data variable also we see 100 dataset names
#with open('superhero/superheroes.txt','r') as f
#  data = f.read()
#data[:100]

Task 2: Data and Tokenizer

we then import tensorflow and check its version 
import tensorflow as tf
print (tf.__version__)

Tokenizer converts character into numeric represetation to all the iteams in our vocabulary.
also in below we filter symbols and give spaces/split
tokenizer = tf.keras.preprocessing.text.Tokenizer(
    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',
    split='\n',
)

we then fit the tokenizer along with our data 
tokenizer.fit_on_texts(data)

now code or nlp is able to converts sequence of character to sequence of numbers Char to index and Index to char dictionaries
char_to_index = tokenizer.word_index
index_to_char = dict((v, k) for k, v in char_to_index.items())

print(index_to_char)

Task 3: Names and Sequences

names = data.splitlines()
names[:8]

if we want to convert name into a sequence than we need to use tokenizer

tokenizer.texts_to_sequences(names[0]) #sequence of number is formed of the given name

we make a function to convert names into sequence while eradicating extra dimensions

def name_to_seq(name):
  return [tokenizer.texts_to_sequences(c)[0][0] for c in name]
name_to_seq(names[2])

oposite of converting back the sequence to our name that was initially there

def seq_to_name(seq):
  return ''.join([index_to_char[i] for i in seq if i != 0])
seq_to_name(name_to_seq(names[2]))

Task 4: Creating Examples

#creating list of sequences that we have in our dataset

sequences = []
for name in names:
  seq = name_to_seq(name)
  if len(seq) >= 2:
    sequences += [seq[:i] for i in range(2, len(seq) + 1)]

sequences[:10] # generates the sequences

max_len = max([len(x)for x in sequences])
print(max_len) # tells us the longest name present in our dataset

padded_sequences = tf.keras.preprocessing.sequence.pad_sequences(   # method in tensorflow
    sequences, padding = 'pre', maxlen = max_len
)
print(padded_sequences[0]) # gives us sthe padded_sequences of our first name in dataset at initial phase

padded_sequences.shape # gives padded_sequences of our total dataset

Task 5: Training and Validation Sets

# code below prepares the data for training by separating the input sequences all characters except the last from the target output the last character for each superhero name. The model will be trained to predict the next character (y) based on the previous characters (x).

x, y = padded_sequences[:, :-1], padded_sequences[:, -1]
print(x.shape, y.shape)

# code is responsible for splitting the dataset into training and testing sets

from sklearn.model_selection import train_test_split           #function to split dataset
x_train, x_test, y_train, y_test = train_test_split(x, y)
print(x_train.shape, y_train.shape)
print(x_test.shape, y_test.shape)

# here we are calculating and displaying the size of the "alphabet" (set of unique characters) used in the superhero names
# as of before we have only 28 vocab we add 0 as special kind of vocab for the use

num_chars = len(char_to_index.keys()) + 1
print(num_chars)

Task 6: Creating the Model

# Build the model with explicit input shape
model = Sequential([
    Embedding(input_dim=num_chars, output_dim=8, input_length=max_len - 1),  # Embedding layer
    Conv1D(64, kernel_size=5, strides=1, activation='tanh', padding='causal'),  # Conv1D layer
    MaxPooling1D(pool_size=2),  # MaxPooling layer
    LSTM(32),  # LSTM layer
    Dense(num_chars, activation='softmax')  # Output layer
])

# Compile the model
model.compile(
    loss='sparse_categorical_crossentropy',
    optimizer='adam',
    metrics=['accuracy']
)

# Force TensorFlow to build the model by explicitly running summary
# This ensures the layers are initialized and shape is inferred
model.build(input_shape=(None, max_len - 1))

# Display the model summary to verify the output
model.summary()

Task 7: Training the Model

we train our data through the help of model developed above

h = model.fit(
    x_train, y_train,
    validation_data=(x_test, y_test),
    epochs=50, verbose=2,
    callbacks=[
        tf.keras.callbacks.EarlyStopping(monitor='val_accuracy', patience=3)
    ]
)

Task 8: Generating Names