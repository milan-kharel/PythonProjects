Task 1: Introduction

first we run this to import dataset from "!git clone https://github.com/am1tyadav/superhero"

here we open the file then read the file after which we store it in data variable also we see 100 dataset names
#with open('superhero/superheroes.txt','r') as f
#  data = f.read()
#data[:100]

Task 2: Data and Tokenizer

we then import tensorflow and check its version 
import tensorflow as tf
print (tf.__version__)

Tokenizer converts character into numeric represetation to all the iteams in our vocabulary.
also in below we filter symbols and give spaces/split
tokenizer = tf.keras.preprocessing.text.Tokenizer(
    filters='!"#$%&()*+,-./:;<=>?@[\\]^_`{|}~',
    split='\n',
)

we then fit the tokenizer along with our data 
tokenizer.fit_on_texts(data)

now code or nlp is able to converts sequence of character to sequence of numbers Char to index and Index to char dictionaries
char_to_index = tokenizer.word_index
index_to_char = dict((v, k) for k, v in char_to_index.items())

print(index_to_char)

Task 3: Names and Sequences

names = data.splitlines()
names[:8]

if we want to convert name into a sequence than we need to use tokenizer

tokenizer.texts_to_sequences(names[0]) #sequence of number is formed of the given name

we make a function to convert names into sequence while eradicating extra dimensions

def name_to_seq(name):
  return [tokenizer.texts_to_sequences(c)[0][0] for c in name]
name_to_seq(names[2])

oposite of converting back the sequence to our name that was initially there

def seq_to_name(seq):
  return ''.join([index_to_char[i] for i in seq if i != 0])
seq_to_name(name_to_seq(names[2]))

Task 4: Creating Examples
Task 5: Training and Validation Sets
Task 6: Creating the Model
Task 7: Training the Model
Task 8: Generating Names